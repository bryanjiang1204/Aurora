{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyORWsc0kMiKuG8trpFCO7Uv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bryanjiang1204/aurora/blob/main/Together.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Video**"
      ],
      "metadata": {
        "id": "AiNMQnJenzZf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fvcore\n",
        "!pip install av\n",
        "!pip install pytorchvideo"
      ],
      "metadata": {
        "id": "6fMx70gXn1gY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "Cpv7IcHTodn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'x3d_s'\n",
        "model = torch.hub.load('facebookresearch/pytorchvideo', model_name, pretrained=True)"
      ],
      "metadata": {
        "id": "vzTVlOatrTkN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4f5e9d5-52c1-4b5c-9122-d6f46b453cd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/facebookresearch/pytorchvideo/zipball/main\" to /root/.cache/torch/hub/main.zip\n",
            "Downloading: \"https://dl.fbaipublicfiles.com/pytorchvideo/model_zoo/kinetics/X3D_S.pyth\" to /root/.cache/torch/hub/checkpoints/X3D_S.pyth\n",
            "100%|██████████| 29.4M/29.4M [00:01<00:00, 30.1MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib\n",
        "\n",
        "from torchvision.transforms import Compose, Lambda\n",
        "from torchvision.transforms._transforms_video import CenterCropVideo, NormalizeVideo\n",
        "from pytorchvideo.transforms import ApplyTransformToKey,ShortSideScale, UniformTemporalSubsample\n",
        "from pytorchvideo.data.encoded_video import EncodedVideo\n",
        "\n",
        "mean = [0.5, 0.5, 0.5]\n",
        "std = [0.25, 0.25, 0.25]\n",
        "\n",
        "num_frames = 13\n",
        "sampling_rate = 6\n",
        "frames_per_second = 30\n",
        "\n",
        "transform =  ApplyTransformToKey(\n",
        "    key=\"video\",\n",
        "    transform=Compose(\n",
        "        [\n",
        "            UniformTemporalSubsample(num_frames),\n",
        "            Lambda(lambda x: x/255.0),\n",
        "            NormalizeVideo(mean, std),\n",
        "            ShortSideScale(size=182),\n",
        "            CenterCropVideo(\n",
        "                crop_size=(182, 182)\n",
        "            )\n",
        "        ]\n",
        "    ),\n",
        ")\n",
        "\n",
        "clip_duration = (num_frames * sampling_rate)/frames_per_second\n",
        "\n",
        "url_link = \"https://dl.fbaipublicfiles.com/pytorchvideo/projects/archery.mp4\"\n",
        "video_path = 'archery.mp4'\n",
        "try: urllib.URLopener().retrieve(url_link, video_path)\n",
        "except: urllib.request.urlretrieve(url_link, video_path)\n",
        "\n",
        "video = EncodedVideo.from_path(video_path)\n",
        "\n",
        "video_data = video.get_clip(start_sec=0, end_sec=3)\n",
        "video_data = transform(video_data)\n",
        "\n",
        "inputs = video_data[\"video\"]"
      ],
      "metadata": {
        "id": "_jGcJmS0o6ik",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5348ecaa-c5d9-486b-8168-b9b0b6cc7d6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models.feature_extraction import create_feature_extractor\n",
        "\n",
        "class Video_Embeddings(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        model = torch.hub.load('facebookresearch/pytorchvideo', model_name, pretrained=True).eval()\n",
        "        self.body = create_feature_extractor(model, return_nodes={'blocks.5.pool.post_conv': 'embeddings',})\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.body(x)\n",
        "        return x\n",
        "\n",
        "extractor = Video_Embeddings()\n",
        "flat_layer = extractor(inputs[None,...])\n",
        "\n",
        "y = torch.flatten(flat_layer['embeddings'])\n",
        "y.shape"
      ],
      "metadata": {
        "id": "bFwQuKy_oeeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text**"
      ],
      "metadata": {
        "id": "Yed6FgyXpWiC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "UEASR-p0pYNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "nf-gnsZIpamn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "input_text = \"Wise Gurus chant\"\n",
        "\n",
        "input_ids = torch.tensor([tokenizer.encode(input_text)])\n",
        "with torch.no_grad():\n",
        "    model_output = model(input_ids)\n",
        "bert_embeddings = model_output[0]\n",
        "\n",
        "x = torch.flatten(bert_embeddings)\n",
        "x.shape\n",
        "#padding"
      ],
      "metadata": {
        "id": "0-B0qnMcpcah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Concotanate"
      ],
      "metadata": {
        "id": "inK5njZvq7CQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "z = torch.cat((x,y))\n",
        "z.shape"
      ],
      "metadata": {
        "id": "clF0wnVDqgxC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb6e71c8-ac5a-412c-bfc5-4354db1a8351"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([12800])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Questions:\n",
        "\n",
        "How to get Audio Embeddings.\n",
        "\n",
        "If the number of words spoken is more than 50, the text embeddings length can go as high as 50,000 - 100,000. How to proportionalize. Or perhaps another solution.\n",
        "\n",
        "What is best video model to use. If a video model's original dimensions are [2048,2,2] does flattening to make it [8192] count as an embedding"
      ],
      "metadata": {
        "id": "T-icxdU6ppqJ"
      }
    }
  ]
}